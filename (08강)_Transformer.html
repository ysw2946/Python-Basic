<html><head><meta http-equiv="Content-Type" content="text/html; charset=utf-8"/><title>8. Sequential Models - Transformer</title><style>
/* cspell:disable-file */
/* webkit printing magic: print all background colors */
html {
	-webkit-print-color-adjust: exact;
}
* {
	box-sizing: border-box;
	-webkit-print-color-adjust: exact;
}

html,
body {
	margin: 0;
	padding: 0;
}
@media only screen {
	body {
		margin: 2em auto;
		max-width: 900px;
		color: rgb(55, 53, 47);
	}
}

body {
	line-height: 1.5;
	white-space: pre-wrap;
}

a,
a.visited {
	color: inherit;
	text-decoration: underline;
}

.pdf-relative-link-path {
	font-size: 80%;
	color: #444;
}

h1,
h2,
h3 {
	letter-spacing: -0.01em;
	line-height: 1.2;
	font-weight: 600;
	margin-bottom: 0;
}

.page-title {
	font-size: 2.5rem;
	font-weight: 700;
	margin-top: 0;
	margin-bottom: 0.75em;
}

h1 {
	font-size: 1.875rem;
	margin-top: 1.875rem;
}

h2 {
	font-size: 1.5rem;
	margin-top: 1.5rem;
}

h3 {
	font-size: 1.25rem;
	margin-top: 1.25rem;
}

.source {
	border: 1px solid #ddd;
	border-radius: 3px;
	padding: 1.5em;
	word-break: break-all;
}

.callout {
	border-radius: 3px;
	padding: 1rem;
}

figure {
	margin: 1.25em 0;
	page-break-inside: avoid;
}

figcaption {
	opacity: 0.5;
	font-size: 85%;
	margin-top: 0.5em;
}

mark {
	background-color: transparent;
}

.indented {
	padding-left: 1.5em;
}

hr {
	background: transparent;
	display: block;
	width: 100%;
	height: 1px;
	visibility: visible;
	border: none;
	border-bottom: 1px solid rgba(55, 53, 47, 0.09);
}

img {
	max-width: 100%;
}

@media only print {
	img {
		max-height: 100vh;
		object-fit: contain;
	}
}

@page {
	margin: 1in;
}

.collection-content {
	font-size: 0.875rem;
}

.column-list {
	display: flex;
	justify-content: space-between;
}

.column {
	padding: 0 1em;
}

.column:first-child {
	padding-left: 0;
}

.column:last-child {
	padding-right: 0;
}

.table_of_contents-item {
	display: block;
	font-size: 0.875rem;
	line-height: 1.3;
	padding: 0.125rem;
}

.table_of_contents-indent-1 {
	margin-left: 1.5rem;
}

.table_of_contents-indent-2 {
	margin-left: 3rem;
}

.table_of_contents-indent-3 {
	margin-left: 4.5rem;
}

.table_of_contents-link {
	text-decoration: none;
	opacity: 0.7;
	border-bottom: 1px solid rgba(55, 53, 47, 0.18);
}

table,
th,
td {
	border: 1px solid rgba(55, 53, 47, 0.09);
	border-collapse: collapse;
}

table {
	border-left: none;
	border-right: none;
}

th,
td {
	font-weight: normal;
	padding: 0.25em 0.5em;
	line-height: 1.5;
	min-height: 1.5em;
	text-align: left;
}

th {
	color: rgba(55, 53, 47, 0.6);
}

ol,
ul {
	margin: 0;
	margin-block-start: 0.6em;
	margin-block-end: 0.6em;
}

li > ol:first-child,
li > ul:first-child {
	margin-block-start: 0.6em;
}

ul > li {
	list-style: disc;
}

ul.to-do-list {
	text-indent: -1.7em;
}

ul.to-do-list > li {
	list-style: none;
}

.to-do-children-checked {
	text-decoration: line-through;
	opacity: 0.375;
}

ul.toggle > li {
	list-style: none;
}

ul {
	padding-inline-start: 1.7em;
}

ul > li {
	padding-left: 0.1em;
}

ol {
	padding-inline-start: 1.6em;
}

ol > li {
	padding-left: 0.2em;
}

.mono ol {
	padding-inline-start: 2em;
}

.mono ol > li {
	text-indent: -0.4em;
}

.toggle {
	padding-inline-start: 0em;
	list-style-type: none;
}

/* Indent toggle children */
.toggle > li > details {
	padding-left: 1.7em;
}

.toggle > li > details > summary {
	margin-left: -1.1em;
}

.selected-value {
	display: inline-block;
	padding: 0 0.5em;
	background: rgba(206, 205, 202, 0.5);
	border-radius: 3px;
	margin-right: 0.5em;
	margin-top: 0.3em;
	margin-bottom: 0.3em;
	white-space: nowrap;
}

.collection-title {
	display: inline-block;
	margin-right: 1em;
}

.simple-table {
	margin-top: 1em;
	font-size: 0.875rem;
}

.simple-table-header {
	background: rgb(247, 246, 243);
	color: black;
	font-weight: 500;
}

time {
	opacity: 0.5;
}

.icon {
	display: inline-block;
	max-width: 1.2em;
	max-height: 1.2em;
	text-decoration: none;
	vertical-align: text-bottom;
	margin-right: 0.5em;
}

img.icon {
	border-radius: 3px;
}

.user-icon {
	width: 1.5em;
	height: 1.5em;
	border-radius: 100%;
	margin-right: 0.5rem;
}

.user-icon-inner {
	font-size: 0.8em;
}

.text-icon {
	border: 1px solid #000;
	text-align: center;
}

.page-cover-image {
	display: block;
	object-fit: cover;
	width: 100%;
	max-height: 30vh;
}

.page-header-icon {
	font-size: 3rem;
	margin-bottom: 1rem;
}

.page-header-icon-with-cover {
	margin-top: -0.72em;
	margin-left: 0.07em;
}

.page-header-icon img {
	border-radius: 3px;
}

.link-to-page {
	margin: 1em 0;
	padding: 0;
	border: none;
	font-weight: 500;
}

p > .user {
	opacity: 0.5;
}

td > .user,
td > time {
	white-space: nowrap;
}

input[type="checkbox"] {
	transform: scale(1.5);
	margin-right: 0.6em;
	vertical-align: middle;
}

p {
	margin-top: 0.5em;
	margin-bottom: 0.5em;
}

.image {
	border: none;
	margin: 1.5em 0;
	padding: 0;
	border-radius: 0;
	text-align: center;
}

.code,
code {
	background: rgba(135, 131, 120, 0.15);
	border-radius: 3px;
	padding: 0.2em 0.4em;
	border-radius: 3px;
	font-size: 85%;
	tab-size: 2;
}

code {
	color: #eb5757;
}

.code {
	padding: 1.5em 1em;
}

.code-wrap {
	white-space: pre-wrap;
	word-break: break-all;
}

.code > code {
	background: none;
	padding: 0;
	font-size: 100%;
	color: inherit;
}

blockquote {
	font-size: 1.25em;
	margin: 1em 0;
	padding-left: 1em;
	border-left: 3px solid rgb(55, 53, 47);
}

.bookmark {
	text-decoration: none;
	max-height: 8em;
	padding: 0;
	display: flex;
	width: 100%;
	align-items: stretch;
}

.bookmark-title {
	font-size: 0.85em;
	overflow: hidden;
	text-overflow: ellipsis;
	height: 1.75em;
	white-space: nowrap;
}

.bookmark-text {
	display: flex;
	flex-direction: column;
}

.bookmark-info {
	flex: 4 1 180px;
	padding: 12px 14px 14px;
	display: flex;
	flex-direction: column;
	justify-content: space-between;
}

.bookmark-image {
	width: 33%;
	flex: 1 1 180px;
	display: block;
	position: relative;
	object-fit: cover;
	border-radius: 1px;
}

.bookmark-description {
	color: rgba(55, 53, 47, 0.6);
	font-size: 0.75em;
	overflow: hidden;
	max-height: 4.5em;
	word-break: break-word;
}

.bookmark-href {
	font-size: 0.75em;
	margin-top: 0.25em;
}

.sans { font-family: ui-sans-serif, -apple-system, BlinkMacSystemFont, "Segoe UI", Helvetica, "Apple Color Emoji", Arial, sans-serif, "Segoe UI Emoji", "Segoe UI Symbol"; }
.code { font-family: "SFMono-Regular", Menlo, Consolas, "PT Mono", "Liberation Mono", Courier, monospace; }
.serif { font-family: Lyon-Text, Georgia, ui-serif, serif; }
.mono { font-family: iawriter-mono, Nitti, Menlo, Courier, monospace; }
.pdf .sans { font-family: Inter, ui-sans-serif, -apple-system, BlinkMacSystemFont, "Segoe UI", Helvetica, "Apple Color Emoji", Arial, sans-serif, "Segoe UI Emoji", "Segoe UI Symbol", 'Twemoji', 'Noto Color Emoji', 'Noto Sans CJK JP'; }
.pdf:lang(zh-CN) .sans { font-family: Inter, ui-sans-serif, -apple-system, BlinkMacSystemFont, "Segoe UI", Helvetica, "Apple Color Emoji", Arial, sans-serif, "Segoe UI Emoji", "Segoe UI Symbol", 'Twemoji', 'Noto Color Emoji', 'Noto Sans CJK SC'; }
.pdf:lang(zh-TW) .sans { font-family: Inter, ui-sans-serif, -apple-system, BlinkMacSystemFont, "Segoe UI", Helvetica, "Apple Color Emoji", Arial, sans-serif, "Segoe UI Emoji", "Segoe UI Symbol", 'Twemoji', 'Noto Color Emoji', 'Noto Sans CJK TC'; }
.pdf:lang(ko-KR) .sans { font-family: Inter, ui-sans-serif, -apple-system, BlinkMacSystemFont, "Segoe UI", Helvetica, "Apple Color Emoji", Arial, sans-serif, "Segoe UI Emoji", "Segoe UI Symbol", 'Twemoji', 'Noto Color Emoji', 'Noto Sans CJK KR'; }
.pdf .code { font-family: Source Code Pro, "SFMono-Regular", Menlo, Consolas, "PT Mono", "Liberation Mono", Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK JP'; }
.pdf:lang(zh-CN) .code { font-family: Source Code Pro, "SFMono-Regular", Menlo, Consolas, "PT Mono", "Liberation Mono", Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK SC'; }
.pdf:lang(zh-TW) .code { font-family: Source Code Pro, "SFMono-Regular", Menlo, Consolas, "PT Mono", "Liberation Mono", Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK TC'; }
.pdf:lang(ko-KR) .code { font-family: Source Code Pro, "SFMono-Regular", Menlo, Consolas, "PT Mono", "Liberation Mono", Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK KR'; }
.pdf .serif { font-family: PT Serif, Lyon-Text, Georgia, ui-serif, serif, 'Twemoji', 'Noto Color Emoji', 'Noto Serif CJK JP'; }
.pdf:lang(zh-CN) .serif { font-family: PT Serif, Lyon-Text, Georgia, ui-serif, serif, 'Twemoji', 'Noto Color Emoji', 'Noto Serif CJK SC'; }
.pdf:lang(zh-TW) .serif { font-family: PT Serif, Lyon-Text, Georgia, ui-serif, serif, 'Twemoji', 'Noto Color Emoji', 'Noto Serif CJK TC'; }
.pdf:lang(ko-KR) .serif { font-family: PT Serif, Lyon-Text, Georgia, ui-serif, serif, 'Twemoji', 'Noto Color Emoji', 'Noto Serif CJK KR'; }
.pdf .mono { font-family: PT Mono, iawriter-mono, Nitti, Menlo, Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK JP'; }
.pdf:lang(zh-CN) .mono { font-family: PT Mono, iawriter-mono, Nitti, Menlo, Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK SC'; }
.pdf:lang(zh-TW) .mono { font-family: PT Mono, iawriter-mono, Nitti, Menlo, Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK TC'; }
.pdf:lang(ko-KR) .mono { font-family: PT Mono, iawriter-mono, Nitti, Menlo, Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK KR'; }
.highlight-default {
	color: rgba(55, 53, 47, 1);
}
.highlight-gray {
	color: rgba(120, 119, 116, 1);
	fill: rgba(120, 119, 116, 1);
}
.highlight-brown {
	color: rgba(159, 107, 83, 1);
	fill: rgba(159, 107, 83, 1);
}
.highlight-orange {
	color: rgba(217, 115, 13, 1);
	fill: rgba(217, 115, 13, 1);
}
.highlight-yellow {
	color: rgba(203, 145, 47, 1);
	fill: rgba(203, 145, 47, 1);
}
.highlight-teal {
	color: rgba(68, 131, 97, 1);
	fill: rgba(68, 131, 97, 1);
}
.highlight-blue {
	color: rgba(51, 126, 169, 1);
	fill: rgba(51, 126, 169, 1);
}
.highlight-purple {
	color: rgba(144, 101, 176, 1);
	fill: rgba(144, 101, 176, 1);
}
.highlight-pink {
	color: rgba(193, 76, 138, 1);
	fill: rgba(193, 76, 138, 1);
}
.highlight-red {
	color: rgba(212, 76, 71, 1);
	fill: rgba(212, 76, 71, 1);
}
.highlight-gray_background {
	background: rgba(241, 241, 239, 1);
}
.highlight-brown_background {
	background: rgba(244, 238, 238, 1);
}
.highlight-orange_background {
	background: rgba(251, 236, 221, 1);
}
.highlight-yellow_background {
	background: rgba(251, 243, 219, 1);
}
.highlight-teal_background {
	background: rgba(237, 243, 236, 1);
}
.highlight-blue_background {
	background: rgba(231, 243, 248, 1);
}
.highlight-purple_background {
	background: rgba(244, 240, 247, 0.8);
}
.highlight-pink_background {
	background: rgba(249, 238, 243, 0.8);
}
.highlight-red_background {
	background: rgba(253, 235, 236, 1);
}
.block-color-default {
	color: inherit;
	fill: inherit;
}
.block-color-gray {
	color: rgba(120, 119, 116, 1);
	fill: rgba(120, 119, 116, 1);
}
.block-color-brown {
	color: rgba(159, 107, 83, 1);
	fill: rgba(159, 107, 83, 1);
}
.block-color-orange {
	color: rgba(217, 115, 13, 1);
	fill: rgba(217, 115, 13, 1);
}
.block-color-yellow {
	color: rgba(203, 145, 47, 1);
	fill: rgba(203, 145, 47, 1);
}
.block-color-teal {
	color: rgba(68, 131, 97, 1);
	fill: rgba(68, 131, 97, 1);
}
.block-color-blue {
	color: rgba(51, 126, 169, 1);
	fill: rgba(51, 126, 169, 1);
}
.block-color-purple {
	color: rgba(144, 101, 176, 1);
	fill: rgba(144, 101, 176, 1);
}
.block-color-pink {
	color: rgba(193, 76, 138, 1);
	fill: rgba(193, 76, 138, 1);
}
.block-color-red {
	color: rgba(212, 76, 71, 1);
	fill: rgba(212, 76, 71, 1);
}
.block-color-gray_background {
	background: rgba(241, 241, 239, 1);
}
.block-color-brown_background {
	background: rgba(244, 238, 238, 1);
}
.block-color-orange_background {
	background: rgba(251, 236, 221, 1);
}
.block-color-yellow_background {
	background: rgba(251, 243, 219, 1);
}
.block-color-teal_background {
	background: rgba(237, 243, 236, 1);
}
.block-color-blue_background {
	background: rgba(231, 243, 248, 1);
}
.block-color-purple_background {
	background: rgba(244, 240, 247, 0.8);
}
.block-color-pink_background {
	background: rgba(249, 238, 243, 0.8);
}
.block-color-red_background {
	background: rgba(253, 235, 236, 1);
}
.select-value-color-pink { background-color: rgba(245, 224, 233, 1); }
.select-value-color-purple { background-color: rgba(232, 222, 238, 1); }
.select-value-color-green { background-color: rgba(219, 237, 219, 1); }
.select-value-color-gray { background-color: rgba(227, 226, 224, 1); }
.select-value-color-opaquegray { background-color: rgba(255, 255, 255, 0.0375); }
.select-value-color-orange { background-color: rgba(250, 222, 201, 1); }
.select-value-color-brown { background-color: rgba(238, 224, 218, 1); }
.select-value-color-red { background-color: rgba(255, 226, 221, 1); }
.select-value-color-yellow { background-color: rgba(253, 236, 200, 1); }
.select-value-color-blue { background-color: rgba(211, 229, 239, 1); }

.checkbox {
	display: inline-flex;
	vertical-align: text-bottom;
	width: 16;
	height: 16;
	background-size: 16px;
	margin-left: 2px;
	margin-right: 5px;
}

.checkbox-on {
	background-image: url("data:image/svg+xml;charset=UTF-8,%3Csvg%20width%3D%2216%22%20height%3D%2216%22%20viewBox%3D%220%200%2016%2016%22%20fill%3D%22none%22%20xmlns%3D%22http%3A%2F%2Fwww.w3.org%2F2000%2Fsvg%22%3E%0A%3Crect%20width%3D%2216%22%20height%3D%2216%22%20fill%3D%22%2358A9D7%22%2F%3E%0A%3Cpath%20d%3D%22M6.71429%2012.2852L14%204.9995L12.7143%203.71436L6.71429%209.71378L3.28571%206.2831L2%207.57092L6.71429%2012.2852Z%22%20fill%3D%22white%22%2F%3E%0A%3C%2Fsvg%3E");
}

.checkbox-off {
	background-image: url("data:image/svg+xml;charset=UTF-8,%3Csvg%20width%3D%2216%22%20height%3D%2216%22%20viewBox%3D%220%200%2016%2016%22%20fill%3D%22none%22%20xmlns%3D%22http%3A%2F%2Fwww.w3.org%2F2000%2Fsvg%22%3E%0A%3Crect%20x%3D%220.75%22%20y%3D%220.75%22%20width%3D%2214.5%22%20height%3D%2214.5%22%20fill%3D%22white%22%20stroke%3D%22%2336352F%22%20stroke-width%3D%221.5%22%2F%3E%0A%3C%2Fsvg%3E");
}
	
</style></head><body><article id="6d7783bf-f20d-430d-a5bf-36472c19e562" class="page sans"><header><h1 class="page-title">8. Sequential Models - Transformer</h1></header><div class="page-body"><p id="77ef74f7-6fef-4ce6-a90b-8288a698ef8a" class="">
</p><h3 id="f49d67ea-d3f0-4a46-853c-90edc47508be" class="">Sequential Model</h3><hr id="3d86f90b-7742-419e-b067-691a1f2c3a88"/><figure id="b4e183df-1163-4ba4-a2c3-c782829234f8" class="image"><a href="8%20Sequential%20Models%20-%20Transformer%20b4e183df11634ba4a2c3c782829234f8/Untitled.png"><img style="width:480px" src="8%20Sequential%20Models%20-%20Transformer%20b4e183df11634ba4a2c3c782829234f8/Untitled.png"/></a></figure><ul id="88ab9cad-77a9-4d67-80ac-e667bf9f57af" class="bulleted-list"><li style="list-style-type:disc">문장은 항상 길이가 달라질 수 있으며 어순이나 바뀌거나 단어가 빠지는 문제들이 있기 때문에 모델링하기가 어렵다.</li></ul><p id="fabdee68-1316-482a-a061-c715809c61dd" class="">
</p><ul id="ca5395db-3e8b-452c-b005-cc5cd14afea4" class="bulleted-list"><li style="list-style-type:disc">이러한 문제들을 해결하고자 Transformer self attention이라는 구조를 사용한다</li></ul><p id="2c768d52-1631-4ac0-9f1a-c7c8202e69a0" class="">
</p><p id="10355403-a8ee-4ec0-bcc7-92b0fdc7f97b" class="">
</p><h2 id="06f5becc-95ab-425f-b13d-055334da258b" class="">Transformer</h2><hr id="e6f0b7a6-a6f2-40c2-b9e6-c75641e79f00"/><figure id="b017e3f7-8788-426f-9703-e69fb56b54fb" class="image"><a href="8%20Sequential%20Models%20-%20Transformer%20b4e183df11634ba4a2c3c782829234f8/Untitled%201.png"><img style="width:384px" src="8%20Sequential%20Models%20-%20Transformer%20b4e183df11634ba4a2c3c782829234f8/Untitled%201.png"/></a></figure><figure id="0954eb38-5bfe-40ae-af7c-6305890fbbd8" class="image"><a href="8%20Sequential%20Models%20-%20Transformer%20b4e183df11634ba4a2c3c782829234f8/Untitled%202.png"><img style="width:432px" src="8%20Sequential%20Models%20-%20Transformer%20b4e183df11634ba4a2c3c782829234f8/Untitled%202.png"/></a></figure><ul id="27872d31-fc31-4025-8c16-81a3015c51a0" class="bulleted-list"><li style="list-style-type:disc">RNN은 재귀적인 구조인데, <strong>transform은 재귀적인 구조가 없고, attention이라고 불리는 구조를 활용</strong>했다.</li></ul><p id="175f56c6-9c24-47da-98f7-27ec031ba86c" class="">
</p><ul id="fc20c584-6d77-42fc-95b0-45e4b6afab78" class="bulleted-list"><li style="list-style-type:disc">이 방법론은 sequential 데이터를 처리하고 이 데이터를 encoding하는 방법이기 때문에 NMT(Neural Machine Translation)문제에만 적용되지 않는다.</li></ul><p id="e0ab402f-7e91-4f0f-84d9-7753c2f49923" class="">
</p><ul id="ff67a1c7-8477-43d8-a0fd-0d8c7bfe88cf" class="bulleted-list"><li style="list-style-type:disc">따라서, transfromer는 image classification 또는 image detection , visual transformer등에 사용되기도 한다</li></ul><p id="5cfe0548-0898-4af5-a1ce-158c5a009d94" class="">
</p><p id="0ecf76d9-5f88-4f28-9ff2-95ed236da224" class="">
</p><figure id="610f35ff-80d5-4364-9502-7d1716d9265e" class="image"><a href="8%20Sequential%20Models%20-%20Transformer%20b4e183df11634ba4a2c3c782829234f8/Untitled%203.png"><img style="width:336px" src="8%20Sequential%20Models%20-%20Transformer%20b4e183df11634ba4a2c3c782829234f8/Untitled%203.png"/></a></figure><ul id="903b8dde-1049-42b3-81c0-da5db5717d18" class="bulleted-list"><li style="list-style-type:disc">결국, 우리가 하려고 하는 것은 불어 문장이 주어지면 영어 문장으로 바꾸는 것과 같은 seq2seq 모델을 만드는 것이다</li></ul><p id="554c1850-6b8b-43e4-ae4c-6739781b801f" class="">
</p><p id="41df81dc-0a28-41b1-aa6a-be90aac2ff4f" class="">
</p><figure id="17f840a5-5d92-4d82-b9e0-35988bce0ef6" class="image"><a href="8%20Sequential%20Models%20-%20Transformer%20b4e183df11634ba4a2c3c782829234f8/Untitled%204.png"><img style="width:480px" src="8%20Sequential%20Models%20-%20Transformer%20b4e183df11634ba4a2c3c782829234f8/Untitled%204.png"/></a></figure><ul id="d0bd0aa5-87b8-4212-9f28-123b43bfd4ac" class="bulleted-list"><li style="list-style-type:disc">입력은 3개의 단어로 되어있고, 출력은 4개의 단어로 되어있는 것을 보아 입력 시퀀스와 출력 시퀀스의 단어 숫자가 다를 수 있고, 입력 시퀀스의 도메인과 출력 시퀀스의 도메인이 다를 수 있으며 이것이 한 개의 모델로 이루어져 있는 것을 볼 수 있다.</li></ul><p id="dc31742c-2e93-46c8-a7ee-df5bf7b941ef" class="">
</p><ul id="298cf611-01f2-4f98-9bb0-aa6965cd5121" class="bulleted-list"><li style="list-style-type:disc">원래 RNN의 경우는 3개의 단어가 들어가면 3번의 무언가가 돌아가는데, transformer의 encoder는 3개의 단어나 100개의 단어나 상관없이 한번에 100개의 단어를 찍어낼 수 있으며, generateion 할 때는 auto regressive하게 된다.</li></ul><p id="69070d8c-0c9e-4f6c-98fc-1980fbba6a27" class="">
</p><ul id="622adfa1-3a02-4a3a-9bff-5cd512219adf" class="bulleted-list"><li style="list-style-type:disc">즉, <strong>encoder는 한번에 n개의 단어를 한번에 처리할 수 있는 구조</strong>로 이해하면 된다.</li></ul><p id="e460ebb8-72cb-4ad4-ba7f-cbe1d47d5d97" class="">
</p><ul id="c1e50577-6d14-4052-998a-7a70f9fe62ae" class="bulleted-list"><li style="list-style-type:disc">transformer는<strong> encoder와 decorder가 동일한 구조로 stack되어있는 모델</strong>이다.</li></ul><p id="6507d224-5718-43b0-8661-03054f6ef95a" class="">
</p><p id="31a17cb1-09a5-49b8-ba5e-d20bcbdd74b8" class="">
</p><p id="97d1024e-6b91-4701-9457-eafdc4a57f48" class="">이제 알아봐야 할 것</p><hr id="cf6550e0-9f78-4a8e-a3fc-a2e0b1308966"/><ol type="1" id="cdb21d07-8ec5-4c8c-a301-a6883e130970" class="numbered-list" start="1"><li>n개의 단어가 어떻게 encoder에서 처리가 되는지?</li></ol><ol type="1" id="b3cc3b1e-f339-40e5-bd5b-e7932ddb5fdb" class="numbered-list" start="2"><li>encoder와 decoder 사이에 어떤 정보를 주고 받는지?</li></ol><ol type="1" id="391c3c7e-a23b-4745-9ac0-381ae2ccff4e" class="numbered-list" start="3"><li>decoder에서 어떻게 generation 할 수 있을지?</li></ol><p id="5ba93c7d-5615-4d84-a4f5-f909ef19cc59" class="">
</p><p id="d2298246-dff7-4a92-829c-f0714bcc773e" class="">
</p><h3 id="99aecd9f-3034-4f97-a60c-0dd0ddfe1709" class="">Encoder</h3><hr id="06d1b830-65d2-4ac4-994f-3bbf5f3e5676"/><figure id="595347c0-39ca-4f93-b704-8b9e3b3a86b9" class="image"><a href="8%20Sequential%20Models%20-%20Transformer%20b4e183df11634ba4a2c3c782829234f8/Untitled%205.png"><img style="width:992px" src="8%20Sequential%20Models%20-%20Transformer%20b4e183df11634ba4a2c3c782829234f8/Untitled%205.png"/></a></figure><ul id="13192a39-56e0-4663-85df-f559ee84c32b" class="bulleted-list"><li style="list-style-type:disc"><strong>Encoder는 Self attetion과 Feed Forward network가 합쳐진 구조</strong>로, 이 <strong>한 쌍이 하나의 encoder</strong>가 된다.</li></ul><ul id="4e3e6ecb-56af-4aa7-a605-918d15c5e23d" class="bulleted-list"><li style="list-style-type:disc">첫 번째 레이어의 encoder에서 나온 출력값은 두 번째 레이어의 encoder로 들어가게 되고 이것을 반복하는 구조로 이루어져 있다.</li></ul><p id="c8c55588-a6a2-4e76-8d7f-5584b8c19543" class="">
</p><ul id="68ca91f4-eb72-4154-bdee-69035f89090e" class="bulleted-list"><li style="list-style-type:disc"><strong>Self-Attention이 transformer의 key point</strong>이며, feed forward network는 사실 mlp와 동일하다.</li></ul><p id="3848a512-8d6c-454f-9a36-84b00aa6d82c" class="">
</p><p id="bb8d2d40-28da-47ca-b349-4586b837f116" class="">
</p><h3 id="033bf9de-f612-4e2b-8cff-a21028740f3d" class="">Self attention</h3><hr id="6c85dfa8-7687-4a66-8f63-ab4bf47192cb"/><figure id="718db392-a657-4039-aadf-38c914ac0ac7" class="image"><a href="8%20Sequential%20Models%20-%20Transformer%20b4e183df11634ba4a2c3c782829234f8/Untitled%206.png"><img style="width:718px" src="8%20Sequential%20Models%20-%20Transformer%20b4e183df11634ba4a2c3c782829234f8/Untitled%206.png"/></a></figure><ul id="1e6d0389-e6d1-4d15-b178-2981c3c98181" class="bulleted-list"><li style="list-style-type:disc">3개의 단어가 들어오고 기계로 번역하여 특정 숫자의 3개의 벡터로 표현된다.</li></ul><p id="a223720f-1a28-4304-904b-f9d04b7c52f5" class="">
</p><p id="687b3fd6-ed3f-457c-a9eb-b6dfca0084ed" class="">
</p><figure id="356d1b7a-5c0f-41c6-a35b-03dc17af8455" class="image"><a href="8%20Sequential%20Models%20-%20Transformer%20b4e183df11634ba4a2c3c782829234f8/Untitled%207.png"><img style="width:765px" src="8%20Sequential%20Models%20-%20Transformer%20b4e183df11634ba4a2c3c782829234f8/Untitled%207.png"/></a></figure><ul id="0ef27bec-4b9d-425e-9584-b2b370de0e52" class="bulleted-list"><li style="list-style-type:disc">self attention은 3개의 단어가 주어지면 3개의 벡터를 찾아준다.</li></ul><p id="8a153c6f-c93e-487f-957a-b9cb20cafacc" class="">
</p><ul id="18b763e4-d89d-497a-bb31-4e641a62b56a" class="bulleted-list"><li style="list-style-type:disc">여기서 중요한 점은 <strong>하나의 벡터 x1이 z1으로 넘어갈 때 단순히 x1의 정보만 활용하는 것이 아니라 x2와 x3의 정보까지 함께 활용</strong>하는 것</li></ul><p id="919ffd64-1e67-4a06-862e-f73767dc5632" class="">
</p><ul id="33b0263b-aaf0-4e51-8728-e1141cfcceba" class="bulleted-list"><li style="list-style-type:disc">즉, n개의 단어가 주어지고 n개의 z벡터를 찾는데 <strong>각각의 x_i를 z_i로 바꿀 때 나머지 n-1개의 x를 같이 고려를 하는 것</strong>으로 dependency가 존재한다</li></ul><p id="57535acb-a165-47c3-a170-d82b2a7a8db6" class="">
</p><ul id="11abfba3-1dfe-405e-b28e-843474a8bce9" class="bulleted-list"><li style="list-style-type:disc">feed forward에서는 dependency가 존재하지 않고 z 벡터들을 그대로 feed forward를 통과시켜 변환해주는 것</li></ul><p id="f017bf97-0ce6-42cf-98dc-eb797ba99b12" class="">
</p><p id="28f0c44f-6e86-4cd0-a55c-aed2cdc4c6ad" class="">
</p><figure id="323293e6-ce6a-4f0f-aeab-5b7cadae38fa" class="image"><a href="8%20Sequential%20Models%20-%20Transformer%20b4e183df11634ba4a2c3c782829234f8/Untitled%208.png"><img style="width:528px" src="8%20Sequential%20Models%20-%20Transformer%20b4e183df11634ba4a2c3c782829234f8/Untitled%208.png"/></a></figure><ul id="efb3a15e-fbac-46a6-835b-fe162c2354b1" class="bulleted-list"><li style="list-style-type:disc">‘The animal didn’t cross the street because it was too tired.’ 이러한 문장이 주어졌을 때 문장을 이해하기 위해서는 뒤에 나오는 it이 어떤 단어에 dependent하는지 알아야 한다.</li></ul><p id="d43f31fc-c1f9-4a72-9510-8ce16d35619d" class="">
</p><ul id="d25a8055-a486-42ea-909b-19660ea2ee53" class="bulleted-list"><li style="list-style-type:disc">바꿔 말하면, 하나의 문장에서의 단어를 설명할 때는 단어 그 자체로만 이해하는 것이 아니라 <strong>문장 속에서 다른 단어들과 어떤 interaction이 있는지를 이해</strong>해야 한다.</li></ul><p id="02625b3d-0e80-4935-9a51-043d630dba5f" class="">
</p><ul id="42b62dee-7ddf-4364-8b9c-2b19c57e1d3a" class="bulleted-list"><li style="list-style-type:disc">transformer 는 it이라는 단어를 <strong>encoding하게 되면 다른 단어들과의 관계성을 보게 되고</strong>, it이 animal과 높은 관계가 있다고 학습이 되어진다</li></ul><p id="2259c8c5-9770-49b2-a9ac-9a724278ea14" class="">
</p><ul id="9a9170e4-eb1b-4b44-8a2f-f3b03c853cd4" class="bulleted-list"><li style="list-style-type:disc">따라서, 이를 통해 기계가 단어를 더 잘 이해할 수 있다.</li></ul><p id="03f979b3-19dd-4dc3-a34f-1706345b4b84" class="">
</p><p id="d0e15962-a09a-4cee-bc1b-bb1427f0abc9" class="">
</p><p id="34e401a2-2a52-486b-9216-ee40c35896fc" class="">
</p><ul id="ca92f002-0dbd-4eaf-9cb9-8fe773dacea6" class="bulleted-list"><li style="list-style-type:disc">self attention 구조</li></ul><figure id="453ec418-1c6e-4c56-9029-e100b2069d34" class="image"><a href="8%20Sequential%20Models%20-%20Transformer%20b4e183df11634ba4a2c3c782829234f8/Untitled%209.png"><img style="width:480px" src="8%20Sequential%20Models%20-%20Transformer%20b4e183df11634ba4a2c3c782829234f8/Untitled%209.png"/></a></figure><ul id="a672cd36-1b99-40b6-94db-aaa522dfab99" class="bulleted-list"><li style="list-style-type:disc">기본적으로 self attention 구조는 <strong>3가지의 Query,Key,Value 벡터를 만들어 내는데 각 단어마다 Q, K, V가 존재</strong>한다. 3개의 네트워크가 있다고 생각!</li></ul><p id="56e023f6-cfaf-4165-aa84-c8616b591e51" class="">
</p><ul id="68fab350-f4ab-45ab-afd3-7d61695265e1" class="bulleted-list"><li style="list-style-type:disc">이러한 Q,K,V 벡터들은 주어진 입력이 주어졌을 때 하나의 단어당 3개의 벡터를 만들게 되고, x1이라는 첫번째 단어에 대한 <strong>임베딩 벡터를 새로운 벡터로 바꾸어 줄 것</strong>이다.</li></ul><p id="1737aa45-9465-4996-81c4-2bd76cdef47c" class="">
</p><p id="98b3a378-664f-45a2-8c05-e88d39439a94" class="">
</p><figure id="e6aa2c01-5c55-407e-bb99-312a9af5f7cd" class="image"><a href="8%20Sequential%20Models%20-%20Transformer%20b4e183df11634ba4a2c3c782829234f8/Untitled%2010.png"><img style="width:431px" src="8%20Sequential%20Models%20-%20Transformer%20b4e183df11634ba4a2c3c782829234f8/Untitled%2010.png"/></a></figure><ul id="0a0836bb-57f9-4a8b-b726-3d239ea20c66" class="bulleted-list"><li style="list-style-type:disc"><strong>score vector</strong><ul id="34cd09da-0295-4af0-982d-9ea10852cf38" class="bulleted-list"><li style="list-style-type:circle">내가 <strong>encoding하고자 하는 Q vector와 나머지 n개의 단어에 대한 K vector를 구하고 이 두 벡터를 내적</strong>하는 것으로 score vector를 구할 수 있으며, 이는 <strong>i번째 단어가 나머지 단어들과 어떠한 interaction이 있는지 얼마나 유사</strong>한지를 알 수 있다.</li></ul><ul id="b2ff561e-90a6-4f96-8f80-64cd5b44fa63" class="bulleted-list"><li style="list-style-type:circle">위의 사진에서 첫번째 단어 ‘Thinking’에 대한 score vector를 계산하기 위해서는 내가 encoding하고자 하는 q1와 나머지 n개의 단어에 대한 k2의 내적으로 score vector를 나타낼 수 있다.</li></ul></li></ul><p id="3f728d78-50c7-4e44-a3c0-7ff70a189a32" class="">
</p><ul id="eb27f9ed-0ad7-4d51-8e65-9b514153584f" class="bulleted-list"><li style="list-style-type:disc"><strong>score vector를 통해 단어 간 얼마나 interaction 하는지를 학습하게 하는 것이고, 이를 attention </strong>이라 한다.</li></ul><p id="9c89659c-7064-48f4-b7d7-4b02c350fb05" class="">
</p><p id="97e5ec40-9dc1-4084-ac9f-9781be6fc4b0" class="">
</p><ul id="d3264783-e701-4282-9f5a-f5f75362ccda" class="bulleted-list"><li style="list-style-type:disc">interaction score</li></ul><figure id="3e2d1598-1dfc-4824-aade-b39d0f713670" class="image"><a href="8%20Sequential%20Models%20-%20Transformer%20b4e183df11634ba4a2c3c782829234f8/Untitled%2011.png"><img style="width:436px" src="8%20Sequential%20Models%20-%20Transformer%20b4e183df11634ba4a2c3c782829234f8/Untitled%2011.png"/></a></figure><ol type="1" id="c286c4be-d32c-4123-b8e7-d674ebe7f408" class="numbered-list" start="1"><li>score vector를 sqrt(key dimension) 값으로 나눈다.<ul id="39bed5e6-0d26-4d05-bc26-5c0b017a5112" class="bulleted-list"><li style="list-style-type:disc">이렇게 나누는 이유는 Key vector의 dimension에 dependent하기 때문이다. </li></ul><ul id="6e47878f-326a-4b04-9bc1-5df83c5fd929" class="bulleted-list"><li style="list-style-type:disc">여기서는 64 demension 이기 때문에 8로 나눔</li></ul></li></ol><p id="c4d14652-d2c4-4f92-9d99-ebb3d3d0e182" class="">
</p><ol type="1" id="fabfae44-5292-41a0-8935-c56014630d04" class="numbered-list" start="2"><li>normalization된 score가 sum해서 1이 되도록 softmax를 취해주면 각 단어에 대한 interaction 값이 나타난다.<ul id="382d0a30-d097-470f-b1aa-0216294e6970" class="bulleted-list"><li style="list-style-type:disc">Thinking이라는 단어의 자신과의 interaction 값은 0.88이 되고 Machine과의 interaction의 값은 0.12가 된다.</li></ul></li></ol><p id="6ecda804-144d-4b51-8c68-71d7472dd39c" class="">
</p><ol type="1" id="047c2c07-8e39-417d-9423-88692a2a1196" class="numbered-list" start="3"><li>이렇게 만들어진 interaction score와 value vector의 <strong>가중합</strong>을 구하면 최종적으로 각 단어의 인코딩 된 벡터가 나타난다.<ul id="51ca235b-7d6a-46c6-96ee-c50723947173" class="bulleted-list"><li style="list-style-type:disc">각 단어의 임베딩 벡터를 인코딩 벡터로 변환</li></ul></li></ol><p id="d2f76e3e-d7a1-4fe6-93a1-9675ea37fe5a" class="">
</p><h3 id="bfefd596-abcf-42d7-9fc9-95025a2a36cc" class="">self-attention 정리</h3><hr id="e31fd5d3-929d-4131-a7a1-eed8c23534ad"/><ol type="1" id="c134183b-55f8-48a2-902d-31389a03a273" class="numbered-list" start="1"><li>단어에 대한 임베딩 벡터가 있고, Query,key,value vector 생성</li></ol><ol type="1" id="33b68b89-b0a3-4175-bca4-d94abcc813e2" class="numbered-list" start="2"><li>Query vector와 전체 단어에 대한 Key vector의 내적으로 score vector 생성</li></ol><ol type="1" id="ea9fca11-f6d4-4c4e-b7e3-65fb74267a3e" class="numbered-list" start="3"><li>score vector를 key value dimension의 제곱근으로 나눠준 후(normalization) softmax를 취하여 attention 생성</li></ol><ol type="1" id="e4a73c81-5813-4367-95e1-69c8f8c750bf" class="numbered-list" start="4"><li>attention과 value vector 의 가중합으로 최종 인코딩 벡터 생성</li></ol><ol type="1" id="31b7662b-12fb-46fd-8ede-1f1c2023155f" class="numbered-list" start="5"><li>이를 반복함으로 각 단어에 대한 인코딩 벡터 생성</li></ol><hr id="67225601-8e0f-4a3a-9ad9-56000cdc404d"/><p id="9b6a3944-34e3-4e6f-b4a7-310d1eaf188b" class="">
</p><ul id="3dcd241b-edb7-42a3-9e1c-fa5e51a4402c" class="bulleted-list"><li style="list-style-type:disc">여기서 주의해야 할 점<ul id="f4060def-6ca6-4126-8615-aa2e0f70c844" class="bulleted-list"><li style="list-style-type:circle">Q vector와 K vector는 내적을 해야 하기 때문에 항상 차원이 같아야 한다. 하지만, V vector는 가중합만 하기 때문에 차원이 달라도 된다.</li></ul><ul id="56f751f2-4963-4214-9ed7-a40faa9876aa" class="bulleted-list"><li style="list-style-type:circle"><strong>최종적으로 나오는 인코딩 된 벡터의 차원은 V vector의 차원과 동일</strong>하다.</li></ul></li></ul><p id="98d0ba1f-f5f9-4368-8cb2-33883417eef9" class="">
</p><p id="da5a3bd9-85ca-4727-a9b5-1b2ad9e8f13d" class="">
</p><h3 id="bb52831c-ad02-40d6-b7d8-099f6bf122af" class="">self-attention을 행렬로 표현</h3><hr id="33b64045-8af6-4f96-bc58-bb2e68cdd2fb"/><figure id="2919ca45-8b41-4aeb-902e-fc3eff0e9f81" class="image"><a href="8%20Sequential%20Models%20-%20Transformer%20b4e183df11634ba4a2c3c782829234f8/Untitled%2012.png"><img style="width:624px" src="8%20Sequential%20Models%20-%20Transformer%20b4e183df11634ba4a2c3c782829234f8/Untitled%2012.png"/></a></figure><ul id="a94902d9-5df4-4f91-a9b1-0904d4be41df" class="bulleted-list"><li style="list-style-type:disc">위의 그림은 <strong>임베딩 벡터에서 Q,K,V를 만드는 과정</strong>을 담아내고 있으며, 두 개의 단어가 주어졌을때 각각 두 개의 Q,K,V 벡터가 생성</li></ul><p id="ef46b0e2-c87d-4032-b7de-cd08ac0faab1" class="">
</p><ul id="3573391e-ce5e-434b-85e9-1d7d26cc8935" class="bulleted-list"><li style="list-style-type:disc">X 의 2 x 4는 두 개의 단어가 있으며 각 단어의 임베딩 차원이 4인 것을 말한다.</li></ul><p id="5bfd5203-1627-4b06-82c3-d00dd015ddd6" class="">
</p><ul id="201c6fcf-3ead-4bc8-9c86-e23a09279bf0" class="bulleted-list"><li style="list-style-type:disc"><strong>WQ, WK, WV는 Q,K,V 벡터를 찾아내는 각각의 MLP</strong>로 생각하면 되고 , 이 <strong>MLP는 encoding된 단어마다 shared</strong>된다.</li></ul><p id="75b8ea0f-e876-493c-a5ef-d2c2f4460b67" class="">
</p><p id="22c54f01-9cd0-480c-9243-be7247258d93" class="">
</p><figure id="41c0c866-2eb4-44df-b59f-0aa56e6df10e" class="image"><a href="8%20Sequential%20Models%20-%20Transformer%20b4e183df11634ba4a2c3c782829234f8/Untitled%2013.png"><img style="width:822px" src="8%20Sequential%20Models%20-%20Transformer%20b4e183df11634ba4a2c3c782829234f8/Untitled%2013.png"/></a></figure><ul id="ada5749d-b077-4219-83c1-2ee7bed4c92b" class="bulleted-list"><li style="list-style-type:disc">이전에 했던 <strong>인코딩 벡터를 추출하기 위한 방법을 나타낸 것</strong>으로, Q vector와 K vector를 내적하고, Key vector dimension의 제곱근으로 나눠준 후 V vector와 가중합이 이루어지는 과정이다,</li></ul><p id="a874c465-b7bb-4607-ac64-c75fd6d9941b" class="">
</p><p id="0d8d5353-f2d6-4e32-b3aa-a6e02fe18979" class="">
</p><p id="175df841-2888-4f0f-9a96-c13e22ea0e61" class="">
</p><h3 id="118ec14c-46a5-4a1a-9207-216d292604bf" class="">이러한 방법이 왜 이게 잘될까?</h3><hr id="4c80e1a2-067f-483f-83a9-bdef01c9fb24"/><ul id="c7ca6231-86cb-450a-9fb3-336aa2b7a03c" class="bulleted-list"><li style="list-style-type:disc">어떤 이미지 하나가 주어지고, 이 이미지를 MLP 혹은 CNN을 통해 dimension을 바꿀때 input이 fix되면 출력이 고정된다. → 내가 가지고 있는 weight나 conv filter가 고정되어 있기 때문에</li></ul><p id="c8c6cac5-f499-488c-9984-ce5fcb16585b" class="">
</p><ul id="b39601ea-50bc-413f-b3a5-99b273207920" class="bulleted-list"><li style="list-style-type:disc">하지만, <strong>transformer는 input이 고정되어 있다고 하더라도 encoding하고자 하는 단어와 옆에 있는 단어에 따라 encoding된 값이 달라진다</strong>. </li></ul><p id="5eccedf4-0761-45a5-bd54-3d42a176a77d" class="">
</p><ul id="eceac7f3-7447-48e4-9682-d83df708f40e" class="bulleted-list"><li style="list-style-type:disc">이는 <strong>MLP보다 flexible한 모델</strong>으로 생각할 수 있으며, 입력이 고정되더라도 옆에 주어지는 다른 입력들에 따라 출력이 달라질 수 있는 여지를 줌으로 더 많은 것을 표현할 수 있다.</li></ul><p id="ef703aae-db93-4b66-92a4-4cb63e8474d1" class="">
</p><ul id="f298e417-d828-474f-b8c7-487194d1b014" class="bulleted-list"><li style="list-style-type:disc">RNN은 n개의 단어가 주어졌을 때 그만큼 모델을 돌리면 되지만, <strong>transformer는 n개의 단어를 한번에 처리해야하고 연산비용이 </strong><strong><style>@import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')</style><span data-token-index="0" contenteditable="false" class="notion-text-equation-token" style="user-select:all;-webkit-user-select:all;-moz-user-select:all"><span></span><span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>n</mi><mn>2</mn></msup></mrow><annotation encoding="application/x-tex">n^2</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141079999999999em;vertical-align:0em;"></span><span class="mord"><span class="mord mathnormal">n</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141079999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span></span></span></span></span><span>﻿</span></span></strong><strong>에 비례하기 때문에 length에 따라 처리할 수 있는 한계</strong>가 있다.</li></ul><p id="73876048-d1b6-4463-8b54-6149d684c8e6" class="">
</p><p id="520c4324-e57b-4078-8496-b0901ec4e8b8" class="">
</p><p id="66dd577f-e619-436b-83ca-4bee9e8accde" class="">
</p><h3 id="12e389c6-fc83-4227-968d-9fd90c2ff987" class="">Multi-headed attention</h3><hr id="71c531e6-1167-4bf5-8409-c413d5d28e41"/><figure id="6761c71e-fe91-429a-9a68-6576b42af3ff" class="image"><a href="8%20Sequential%20Models%20-%20Transformer%20b4e183df11634ba4a2c3c782829234f8/Untitled%2014.png"><img style="width:384px" src="8%20Sequential%20Models%20-%20Transformer%20b4e183df11634ba4a2c3c782829234f8/Untitled%2014.png"/></a></figure><ul id="d016431d-4e40-4b4d-a76f-505612fb8762" class="bulleted-list"><li style="list-style-type:disc">하나의 입력에 대해서 Q,K,V를 여러 개 만드는 네트워크</li></ul><p id="47d1fd2d-8abf-4e78-ae3b-8b02e6a5c6a2" class="">
</p><figure id="76c74eac-12e3-4c37-9c17-43c723822fad" class="image"><a href="8%20Sequential%20Models%20-%20Transformer%20b4e183df11634ba4a2c3c782829234f8/Untitled%2015.png"><img style="width:432px" src="8%20Sequential%20Models%20-%20Transformer%20b4e183df11634ba4a2c3c782829234f8/Untitled%2015.png"/></a></figure><ul id="e78aa859-9384-4956-b8d8-2dbc02addd25" class="bulleted-list"><li style="list-style-type:disc">이를 통해 <strong>n개의 attention을 반복하게 되면 n개의 encoding된 vector가 나오게 된다. </strong><ul id="b6111284-f2c5-4120-8d2d-7904c6a4610c" class="bulleted-list"><li style="list-style-type:circle">여기서는 8개의 head가 사용되었고, 하나의 임베딩된 벡터가 있으면 8개의 encoding된 결과가 나타난다</li></ul></li></ul><p id="30d58e99-4fc7-4a56-a951-afe95c08d0b8" class="">
</p><ul id="d548ff23-9bb1-4cf4-acc3-c260e19b866a" class="bulleted-list"><li style="list-style-type:disc">여기서 고려해야 할 점은 encoder가 다음 번으로 넘어가기 위해서는 입력과 출력의 차원을 맞춰 주어야 한다. 즉, <strong>임베딩된 dimension과 인코딩되어서 나오는 vector가 항상 같은 차원이어야 한다</strong></li></ul><p id="e582988a-523a-4660-8ad8-25c2af94548d" class=""> </p><p id="279ace89-74a4-4371-aaec-b3d23d209242" class="">
</p><figure id="a7bae7a7-4740-4773-a7e3-c9cc4450cf08" class="image"><a href="8%20Sequential%20Models%20-%20Transformer%20b4e183df11634ba4a2c3c782829234f8/Untitled%2016.png"><img style="width:432px" src="8%20Sequential%20Models%20-%20Transformer%20b4e183df11634ba4a2c3c782829234f8/Untitled%2016.png"/></a></figure><ul id="ca68ed69-2c3f-4638-9cda-df10752fe95e" class="bulleted-list"><li style="list-style-type:disc">이 때 <strong>선형변환을 통해 차원을 맞추어 주는데</strong>, input이 10 dimension이고 8개의 output이 나타났다고 하면,  80 dimension의 encoding된 vector가 있다고 할 수 있고, 이에 80 x 10행렬을 곱해서 줄여버린다 라고 생각하면 된다.</li></ul><p id="e5d7b55b-91dd-44df-b39a-401968db87e4" class="">
</p><figure id="8d64cc49-8430-47b5-a48b-e080af43f15b" class="image"><a href="8%20Sequential%20Models%20-%20Transformer%20b4e183df11634ba4a2c3c782829234f8/Untitled%2017.png"><img style="width:432px" src="8%20Sequential%20Models%20-%20Transformer%20b4e183df11634ba4a2c3c782829234f8/Untitled%2017.png"/></a></figure><p id="54357b58-92d4-40e9-8987-fb8665b7f30b" class="">
</p><h3 id="85fb09bd-180d-4947-91ce-4dc6314f4c7c" class="">Multi-head attention  정리</h3><hr id="7f5d5fe0-7fe8-47b9-a233-dbb707c3cc19"/><ol type="1" id="ff102e6e-ade9-4867-9ebd-56fb952b8029" class="numbered-list" start="1"><li>s x s 모양인 input이 들어오게 되면 각각의 단어를 임베딩한다</li></ol><ol type="1" id="b82b65fc-7ca8-4f79-96d0-8726ce243aa6" class="numbered-list" start="2"><li>n개의 head로 나누어 n개의 self attention을 통해 n개의 encoding vector를 만든다</li></ol><ol type="1" id="8c4a2ce8-6e8b-40ba-b9e3-ee7fced3dd12" class="numbered-list" start="3"><li>다시 선형변환을 통해 s x s 차원을 맞춘다</li></ol><hr id="f4c2c5b8-c6d7-426f-9030-81b44bfc245f"/><p id="a48d7fae-8abf-4236-8e7d-e7b2e1ce564c" class="">
</p><p id="c9bee3a5-d377-423c-bd12-08c1d2158eeb" class="">
</p><p id="8b68e91e-73ca-4994-b213-c092ad1662d2" class="">
</p><h3 id="aaa3fca1-919d-415a-90da-52d1b68e9e87" class="">Position encoding</h3><hr id="e6815840-d363-4b26-9b8e-62d252c8049b"/><figure id="76ba497d-36e1-4ae7-a715-f6a08210a20e" class="image"><a href="8%20Sequential%20Models%20-%20Transformer%20b4e183df11634ba4a2c3c782829234f8/Untitled%2018.png"><img style="width:480px" src="8%20Sequential%20Models%20-%20Transformer%20b4e183df11634ba4a2c3c782829234f8/Untitled%2018.png"/></a></figure><figure id="684ae0ea-5946-4915-b069-b14ab47f3659" class="image"><a href="8%20Sequential%20Models%20-%20Transformer%20b4e183df11634ba4a2c3c782829234f8/Untitled%2019.png"><img style="width:528px" src="8%20Sequential%20Models%20-%20Transformer%20b4e183df11634ba4a2c3c782829234f8/Untitled%2019.png"/></a></figure><ul id="f1ddab4d-264a-4984-91ce-dea3d0e5c781" class="bulleted-list"><li style="list-style-type:disc">입력에 특정 값을 더해주는 bias라 생각하면 된다</li></ul><ul id="9cb5e02c-0202-49b2-8424-f4fd46761608" class="bulleted-list"><li style="list-style-type:disc">n개의 단어를 sequential하게 넣어줬다고 하지만, sequential의 정보가 이 안에 포함되어 있지 않다</li></ul><ul id="a77758f4-393a-4d3f-9c07-9fce616cc9a3" class="bulleted-list"><li style="list-style-type:disc">그렇기 때문에 문장 혹은 이미지에는 순서가 중요한데, position encoding이 필요하다.</li></ul><p id="a3fb123f-a583-4147-9b48-98448a70043f" class="">
</p><p id="0d263eac-3a6f-438a-ae48-027813d97073" class="">
</p><p id="1b481412-0f8f-4932-9538-fffcd0b92a6f" class="">
</p><p id="199001f6-0074-4a7e-92b6-9e3ed7b161a4" class="">
</p><p id="010347f8-679f-4e7c-9427-6edbe6031dd2" class="">
</p><ul id="023b2a68-65f2-4553-a17d-36d03687a64f" class="bulleted-list"><li style="list-style-type:disc">transformer에서의 encoder 구조</li></ul><figure id="6ef16649-8f69-4b51-b978-d834e4d44eb2" class="image"><a href="8%20Sequential%20Models%20-%20Transformer%20b4e183df11634ba4a2c3c782829234f8/Untitled%2020.png"><img style="width:480px" src="8%20Sequential%20Models%20-%20Transformer%20b4e183df11634ba4a2c3c782829234f8/Untitled%2020.png"/></a></figure><figure id="04c103d0-cb3b-4c8d-9606-8c0a12ccf262" class="image"><a href="8%20Sequential%20Models%20-%20Transformer%20b4e183df11634ba4a2c3c782829234f8/Untitled%2021.png"><img style="width:480px" src="8%20Sequential%20Models%20-%20Transformer%20b4e183df11634ba4a2c3c782829234f8/Untitled%2021.png"/></a></figure><p id="0be33689-99c3-46b6-8f3f-39fb7e70dd63" class="">
</p><ul id="519699f4-5dd7-44b4-bd68-3cac0c82c1a9" class="bulleted-list"><li style="list-style-type:disc">각 단어에 대한 임베딩 벡터는 self attention , add &amp; normalize ,layer norm 를 거치고, 그렇게 생성된 각각의 Z(encoding된) vector에 대해 독립적으로 동일한 네트워크가 동작하는 것을 반복하는 구조이다.</li></ul><p id="288d7eac-b669-48f1-ab7e-535931f55a36" class="">
</p><p id="7b84c942-e211-4828-a536-09e8c8175571" class="">
</p><p id="05323155-cdf6-49cb-9502-15910f97133c" class="">
</p><h3 id="3fb698ba-58f2-4da7-9d33-523742efee89" class="">Decoder</h3><hr id="7868848c-887a-4584-835a-181cd4970eb0"/><figure id="aee5eb99-9025-4f57-ae07-263e859c86bd" class="image"><a href="8%20Sequential%20Models%20-%20Transformer%20b4e183df11634ba4a2c3c782829234f8/Untitled%2022.png"><img style="width:432px" src="8%20Sequential%20Models%20-%20Transformer%20b4e183df11634ba4a2c3c782829234f8/Untitled%2022.png"/></a></figure><p id="1aa6e66d-91cb-491a-9564-8c89ac8b836a" class="">
</p><ul id="0bca3f96-9b3a-4c1f-9e05-9a2cc04abbe0" class="bulleted-list"><li style="list-style-type:disc">Encoder는 주어진 단어를 표현하는 것이고 Decoder는 생성하는 것</li></ul><p id="7825fb01-c6f9-45f9-8350-2c387aec8560" class="">
</p><ul id="872707f2-07cf-4579-9e53-3cbb15ada745" class="bulleted-list"><li style="list-style-type:disc">중요한 것은 <strong>Encoder에서 Decoder로 어떠한 정보가 전해지는 것</strong>인가?</li></ul><p id="9a21dc0c-0829-4d16-9d4c-2fe48d483b98" class="">
</p><ul id="a3165176-1bfe-4404-a236-b8e5a7ed9176" class="bulleted-list"><li style="list-style-type:disc">input에 있는 단어들을 decoder에 있는 출력하고자 하는 단어에 대한 attention map을 만들려면 Key, Value vector가 필요하다</li></ul><p id="6a10337b-833f-4162-ad78-e43d22bf05c6" class="">
</p><ul id="c1ee6d1e-20ce-4917-8e11-2af4c5d789d9" class="bulleted-list"><li style="list-style-type:disc">이 때의 Key,Value vector는 encoder가 stack이 되어있으므로 가장 상위의 encoder에서 나온 K,V vector를 사용한다.</li></ul><p id="a2c30bd5-c7ae-4df9-815e-8fe7ddd3a63a" class="">
</p><figure id="501a74c5-61ed-4006-a490-7407c2c7adc6" class="image"><a href="8%20Sequential%20Models%20-%20Transformer%20b4e183df11634ba4a2c3c782829234f8/Untitled%2023.png"><img style="width:849px" src="8%20Sequential%20Models%20-%20Transformer%20b4e183df11634ba4a2c3c782829234f8/Untitled%2023.png"/></a></figure><ul id="f522005d-2108-4f1b-882b-1946858c49f4" class="bulleted-list"><li style="list-style-type:disc">학습할 때는 입력과 출력의 정답을 알고 있는데, 내가 i번째 단어를 만드는데 모든 문장을 다 알고 있으면 학습하는 의미가 없으므로 <strong>masking을 통해 이전 단어들만 dependent하고 뒤에 단어들에 대해서는 dependent하지 않게 한다.</strong></li></ul><p id="e35f8681-c2d9-4c7c-9a77-ee57f7770671" class="">
</p><ul id="dca24c1c-dc96-42ac-b899-fa4a613b4c22" class="bulleted-list"><li style="list-style-type:disc"><strong>Decoder는 이전까지의 generation 단어들만으로 Qurey를 만들고 encoder로 얻는 Key, Value 벡터들을 활용하여 출력을 나타낸다</strong></li></ul><p id="bc5dd0f3-39bb-4118-bb90-1d149525dca5" class="">
</p><ul id="d161c575-9b53-4486-bdde-2969ed0b47d5" class="bulleted-list"><li style="list-style-type:disc">final layer에는 단어들의 분포를 만들어 그 중 하나씩 sampling하는 것으로 작동한다.</li></ul><p id="6871db17-d8ad-4ece-b68a-124af2e416e7" class="">
</p><p id="aa4bda07-e83e-46f2-a614-042b7202990f" class="">
</p><h2 id="cdcec4f6-e681-44eb-add2-44e093636bf2" class="">VIT</h2><hr id="5f1448de-38aa-49b7-8019-e99ab635272f"/><figure id="e1ee622f-c683-4fff-92a5-bb9390d26875" class="image"><a href="8%20Sequential%20Models%20-%20Transformer%20b4e183df11634ba4a2c3c782829234f8/Untitled%2024.png"><img style="width:480px" src="8%20Sequential%20Models%20-%20Transformer%20b4e183df11634ba4a2c3c782829234f8/Untitled%2024.png"/></a></figure><ul id="c48cb7c3-afc0-4c76-8549-c8de167f2098" class="bulleted-list"><li style="list-style-type:disc">transformer 는 사실 NMT 문제에만 활용이 되었는데, self attention 구조를 이미지 도메인에서도 활용하는 VIT가 나타났다.</li></ul><p id="1da61c0c-0b6a-48c4-9507-7fa4563aa4b0" class="">
</p><ul id="53c9fbe4-9bc2-4e0c-b2ca-181e506a5122" class="bulleted-list"><li style="list-style-type:disc">원래 NMT에서는 문장들이 주어지므로 Sequence가 주어지는데, 여기서는 <strong>이미지를 여러 영역으로 나누고, 각 영역에 있는 subpatch들을 Linear layer를 통과시킴으로써 하나의 입력으로 취급하는 것에 차이가 있다</strong>.</li></ul><p id="35dca0fa-feb6-480a-8a3c-0e26bd83f3e4" class="">
</p></div></article></body></html>